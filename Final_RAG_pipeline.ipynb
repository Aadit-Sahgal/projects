{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvsLMe2CgXm1"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF for PDF parsing\n",
        "import re\n",
        "\n",
        "# llama-index imports\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.core import Document, VectorStoreIndex\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import CompactAndRefine\n",
        "\n",
        "\n",
        "pdf_path = \"sample_contract.pdf\"   # can change this\n",
        "\n",
        "\n",
        "def process_and_index_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_text = \"\\n\".join(page.get_text() for page in doc)\n",
        "    document = Document(text=full_text, metadata={\"source\": pdf_path})\n",
        "    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "    index = VectorStoreIndex.from_documents([document], embed_model=embed_model)\n",
        "    return index, [document]\n",
        "\n",
        "\n",
        "llm = LlamaCPP(\n",
        "    model_path=\"/content/mistral.gguf\",\n",
        "    temperature=0.0,\n",
        "    max_new_tokens=200,\n",
        "    context_window=4096,\n",
        ")\n",
        "\n",
        "\n",
        "def build_rag_pipeline(index, docs, llm):\n",
        "    # Dense embedding retriever\n",
        "    embed_retriever = index.as_retriever(similarity_top_k=3)\n",
        "    # Sparse BM25 retriever\n",
        "    bm25_retriever = BM25Retriever(documents=docs)\n",
        "    # Fusion retriever: combines both and performs query expansion\n",
        "    fusion_retriever = QueryFusionRetriever(\n",
        "        retrievers=[embed_retriever, bm25_retriever],\n",
        "        llm=llm,\n",
        "        similarity_top_k=3,\n",
        "        num_queries=3,\n",
        "        mode=\"reciprocal_rerank\"\n",
        "    )\n",
        "    synthesizer = CompactAndRefine(llm=llm, verbose=True)\n",
        "    return RetrieverQueryEngine(\n",
        "        retriever=fusion_retriever,\n",
        "        response_synthesizer=synthesizer\n",
        "    )\n",
        "\n",
        "\n",
        "index, docs = process_and_index_pdf(pdf_path)\n",
        "rag_engine   = build_rag_pipeline(index, docs, llm)\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"\\nChatbot: Goodbye! Have a great day!\")\n",
        "        break\n",
        "    try:\n",
        "        response = rag_engine.query(user_input)\n",
        "        print(f\"\\nChatbot: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {e}\")\n",
        "        print(\"Please try again or check your setup/API key\")"
      ]
    }
  ]
}